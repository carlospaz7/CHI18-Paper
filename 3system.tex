%!TEX root=paper.tex

% \newpage
\section{A Minimum Viable Language Textbook}
\label{sec:system}

Our long term vision, of an ecosystem where various educational applications, created by different authors, interacting and sharing information in order to maximize the efficiency and enjoyment of the vocabulary improvement process is described in more detail elsewhere \cite{Lungu16}. 

Figure \ref{fig:architecture} highlights two types of applications that are relevant for implementing a language textbook: the {\bf interactive reader apps} allow the learners to interact with texts in their preferred context (e.g. eBooks, News, Blogs), and the {\bf vocabulary trainer apps} allow the readers to practice vocabulary exercises. 
% 
The figure also presents several critical components of the ecosystem with which the applications interact: the learner model, the translation service, the content recommender, and vocabulary recommender. Before we convince other system creators to join such an ecosystem, we have decided to build a {\em minimum viable ecosystem} which includes basic implementations of the core components. 

In this section we briefly describe the various back-end components, and in the next we describe the user interface of a unified, web-based reader and trainer app. The back-end services are implemented using Python. The front-end uses Javascript and HTML5. The source code for both is available online as open-source (the repo url is at the end of this paper).

\begin{figure}[h!]
\centering
  \includegraphics[width=0.60\columnwidth]{figures/zeeguu-architecture.pdf}
  \caption{The architecture of the envisioned software ecosystem}~\label{fig:architecture}
\end{figure}

\subsection{Learner Model}

At the core of the ecosystem a ``learner model'' tracks the evolving knowledge of the learner. 
Based on the learner model, algorithms can make recommendations for the individual applications regarding interesting content to read and appropriate vocabulary to study. The individual applications, in turn, report back to the learner model events from which the learner progress can be infered. 

Currently, the model tracks the probabilities of a user knowing words based on interaction events in the apps: asking for a translation, repeatedly encountering a given word without asking for a translation, or answering an vocabulary exercise.


\subsection{The Translation Service}

The Translation Service is a subsystem implemented using Python which provides translations to all the applications in the ecosystem. Instead of implementing our own contextual translation engine, we rely on existing industrial grade translation APIs. To avoid depending on a single service and to also increase the likelihood that at least one of the alternative translations is the correct one, the translation service collects in parallel results from three third party translation APIs: Google Translate, Microsoft Translate, and Glosbe\footnote{Google and Microsoft provide context-aware translations and multi-word translations. Glosbe is a simple dictionary}. \cite{Jager17-mux} In the next section we explain how the best guess is inserted in the text while the alternatives are available for the readers to consult.

The dependency of the translation service on multiple third party APIs allows for a higher reliability and a chance to guarantee a low response time: when a service is down or too slow to respond, the results from it are ignored. We detail elsewhere the strategies we use to keep response times low\cite{Jager17-mux}.

It is critical that the translation service be used by all the applications in the ecosystem since this allows the server to track the words and the context in which they are being looked up. This information is then used for estimating learner knowledge, for generating personalized recommendations, but also for allowing a teacher to gain insight into student activity.


% by interacting with a core API that provides the basic contextual translations, user knowledge estimation, and recommendations for words to be studied and texts to be read \cite{Lungu16}. 

\subsection{Vocabulary Recommender}

The goal of the vocabulary recommender is to program optimally-timed words to practice. 
% 
To schedule the words to practice the system uses an adaptive, response-time-based scheduling algorithm aimed introduced by Mettler et al. \cite{Mettler14-ARTS}. 

% After evaluating several alternative scheduling strategies we settled on the Mettler one since it has been proven to have gains with both familiar, seen items as well as with new, unseen instances and the benefits of adaptive scheduling were present at an immediate test as well as at a delay \cite{Mettler14-ARTS}. 

The words scheduled for practice come from those that are translated by the readers. However, only a subset of words are actually scheduled for practice: those that are deemed {\em fit for study}. Not fit for study are words that can not be found in frequency lists of the language, expressions which are longer than three words, words whose translation is the same as the origin (e.g. digital(en) = digital(de)), and words whose context is too long. 
% A user can force a word to be considered fit for study by {\em starring} it in the user interface.


\subsection{Content Recommender}

The content recommender aims to present the reader with texts that are both interesting and accessible at the same time. The current implementation requires the reader to select online sources (e.g. news, blogs) to be followed. The sources are constantly scanned for the latest articles and cached by using a custom-made library\footnote{Open sourced at: https://github.com/zeeguu-ecosystem/watchmen}. To add a new source, the teacher of the class (or the admin of the system) only has to add the url of the source.
% The user interface for the article selection is presented in the next section. 

The difficulty of a text is computed by aggregating the individual difficulty of its words. Individual word difficulty can vary from 0 to 10 and is computed in the following way: 
% taking into account the ranking of words in the target language based on frequency combined with the past history of a given user's activity: 

\begin{itemize}
	\item When the word is estimated to be known, its difficulty is considered to be zero. A word can be estimated to be known either based on past readings (i.e. encountered multiple times, but never looked up) or based on past vocabulary exercises (i.e. correctly identified in the most recent exercises) 
	\item When the word is in the top 50K most frequent words in the target language, it's difficulty is 	considered to increase with 0.1 for every 500 words; if the word is not in top 50K, its difficulty is ten.
\end{itemize}

With these strategies for computing word difficulty, the text difficulty is computed as the median of the words in the text. One limitation of this measure of difficulty is that it does not take into account the phrase length, as other measures do. \cite{Kincaid75-Readability}


\newpage
\section{A Web-Based Reader and Trainer Platform}
% as we is composed of instantiations of the components. We present them in turn in this section, focusing the discussion on three key activities that a user of such an interactive textbook is interested in: 

% \begin{enumerate}
	% \item finding texts to read
	% \item reading the found texts
	% \item practicing vocabulary in context
% \end{enumerate}

 % interacts with: the reader and the trainer applications. 
% \begin{added}
	
	In this section we present the user interface of the prototype {\em personalized language textbook} that we have built. It combines in a single responsive web application a reader applications and a voabulary trainer with multiple exercise types, and thus, can be used from a variety of devices. In the user evaluation reported in this paper, it was used from Windows, Android, and iOS devices. Although not presented here, since it was not used in the user evaluation, a smartwatch application also exists as another vocabulary trainer \cite{Nien16-time}.

% \end{added}




\input{3.1.reader}
\input{3.2.trainer}


% \subsection{Translator Service}

% The core API of the ecosystem provides translations. The main advantage of this indirection is that this allows the server to track the words that are looked up and the context (sentence) in which they are being looked up. This information is then used for estimating learner knowledge and for generating personalized exercises. 

% The Translation Service is an API implemented using Python. Instead of implementing our own contextual translation engine, we decided to rely on existing industrial grade translation APIs. To avoid depending on a single service and to also increase the likelihood that at least one of the alternative translations is the correct one, the translation service dispatches in parallel requests to at least three third party translation APIs: Google Translate, Microsoft Translate, and Glosbe. \cite{Jager17-mux} The first two provide contextual translations and multi-word translations, while the third is a simple dictionary. 

% The dependency of the translation service on multiple third party APIs allows for a higher reliability and a chance to guarantee a low response time: when a service is down or too slow to respond, the results from it are ignored. We detail elsewhere the strategies we use to keep response times low\cite{Jager17-mux}.


% \begin{figure}[h!]
% \centering
%   \includegraphics[width=\columnwidth]{figures/teacher_dashboard.png}
%   \caption{A teacher can see the log of the words that a student looked up, their chosen translations, and the corresponding contexts}{
%   \label{fig:teacher}
%   }
% \end{figure}




